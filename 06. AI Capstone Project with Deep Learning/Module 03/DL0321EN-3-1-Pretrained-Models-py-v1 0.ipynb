{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skillsnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f772710980c42cb984f08f091de8793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading concrete_data_week3.zip:   0%|          | 0/97863179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db28f195fccf4bb496729d1122bc28a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 30,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1801: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 14:37:00.560073: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2024-12-09 14:37:00.583074: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394305000 Hz\n",
      "2024-12-09 14:37:00.583627: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fcd57f8e90 executing computations on platform Host. Devices:\n",
      "2024-12-09 14:37:00.583665: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2024-12-09 14:37:00.665644: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3665: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7f3162bcdc50>,\n",
       " <keras.layers.core.Dense at 0x7f3160905f90>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f31cbfb5090>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f31ccb82410>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31c42608d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31c62c6b50>,\n",
       " <keras.layers.core.Activation at 0x7f31c62797d0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f31c599b510>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31c5975b90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31c5830dd0>,\n",
       " <keras.layers.core.Activation at 0x7f31c5830d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31c5845250>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31c57c5350>,\n",
       " <keras.layers.core.Activation at 0x7f31c570e5d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31c56e1e90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31c4142c10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31c41e63d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31c40f9190>,\n",
       " <keras.layers.merge.Add at 0x7f31c40f9fd0>,\n",
       " <keras.layers.core.Activation at 0x7f31c404acd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31bc7b6e90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31bc7a3ad0>,\n",
       " <keras.layers.core.Activation at 0x7f31bc78f210>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31bc724310>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31bc6a1190>,\n",
       " <keras.layers.core.Activation at 0x7f31bc6a1350>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31bc5bb610>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31bc5b3a10>,\n",
       " <keras.layers.merge.Add at 0x7f31bc55ae10>,\n",
       " <keras.layers.core.Activation at 0x7f31bc4d6b50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31bc486910>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31bc4b7f50>,\n",
       " <keras.layers.core.Activation at 0x7f31bc4b7b10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31bc3d0ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31bc34a190>,\n",
       " <keras.layers.core.Activation at 0x7f31bc34a250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31bc2e7950>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31bc25f8d0>,\n",
       " <keras.layers.merge.Add at 0x7f31bc1fa110>,\n",
       " <keras.layers.core.Activation at 0x7f31bc182ad0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31bc10bdd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31bc178a10>,\n",
       " <keras.layers.core.Activation at 0x7f31bc162090>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31bc07ed90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31bc074090>,\n",
       " <keras.layers.core.Activation at 0x7f31bc074250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31c5a43150>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f319c70dbd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f319c70df10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f319c660e90>,\n",
       " <keras.layers.merge.Add at 0x7f319c5ab450>,\n",
       " <keras.layers.core.Activation at 0x7f319c540f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f319c48afd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f319c4bd7d0>,\n",
       " <keras.layers.core.Activation at 0x7f319c4bde50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f319c45ecd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f319c3e91d0>,\n",
       " <keras.layers.core.Activation at 0x7f319c3bf6d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f319c358e90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f319c2bdb50>,\n",
       " <keras.layers.merge.Add at 0x7f319c2bdf50>,\n",
       " <keras.layers.core.Activation at 0x7f319c270210>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f319c210bd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f319c1d2510>,\n",
       " <keras.layers.core.Activation at 0x7f319c1e9e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f319c10ad10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f319c0a4410>,\n",
       " <keras.layers.core.Activation at 0x7f319c0ecad0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31847c5890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f318470ea90>,\n",
       " <keras.layers.merge.Add at 0x7f3184744190>,\n",
       " <keras.layers.core.Activation at 0x7f31846dc390>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f318466e0d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3184653510>,\n",
       " <keras.layers.core.Activation at 0x7f3184653fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3184579cd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31845123d0>,\n",
       " <keras.layers.core.Activation at 0x7f3184557150>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31844db650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3184411250>,\n",
       " <keras.layers.merge.Add at 0x7f3184470550>,\n",
       " <keras.layers.core.Activation at 0x7f3184389050>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31843898d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f318436ca50>,\n",
       " <keras.layers.core.Activation at 0x7f31842ffbd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31842a5c50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f318422be90>,\n",
       " <keras.layers.core.Activation at 0x7f3184201550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f318419ad90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31840b3510>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3184107fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f318406ac90>,\n",
       " <keras.layers.merge.Add at 0x7f318406af50>,\n",
       " <keras.layers.core.Activation at 0x7f3168778cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f316869c510>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31686f5c50>,\n",
       " <keras.layers.core.Activation at 0x7f31686f5110>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3168629390>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3168589dd0>,\n",
       " <keras.layers.core.Activation at 0x7f3168589a90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3168528d50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31684a04d0>,\n",
       " <keras.layers.merge.Add at 0x7f31684a0790>,\n",
       " <keras.layers.core.Activation at 0x7f31683bc8d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31683bcbd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31683b5b50>,\n",
       " <keras.layers.core.Activation at 0x7f31683b5e90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31682cf890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31682177d0>,\n",
       " <keras.layers.core.Activation at 0x7f3168250110>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31681e8250>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f316814db50>,\n",
       " <keras.layers.merge.Add at 0x7f316814d490>,\n",
       " <keras.layers.core.Activation at 0x7f31680e9d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31682cfd90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31680607d0>,\n",
       " <keras.layers.core.Activation at 0x7f3168060f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3163f62d10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3163ee9690>,\n",
       " <keras.layers.core.Activation at 0x7f3163ec35d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3163e5aed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3163dc8b10>,\n",
       " <keras.layers.merge.Add at 0x7f3163dc8f10>,\n",
       " <keras.layers.core.Activation at 0x7f3163d711d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3163f62550>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3163cd69d0>,\n",
       " <keras.layers.core.Activation at 0x7f3163ceaed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3163c0aad0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3163b83210>,\n",
       " <keras.layers.core.Activation at 0x7f3163b4f510>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3163b1ecd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3163aaf190>,\n",
       " <keras.layers.merge.Add at 0x7f3163a875d0>,\n",
       " <keras.layers.core.Activation at 0x7f3163a1de50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31639d4250>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3163996f50>,\n",
       " <keras.layers.core.Activation at 0x7f3163996290>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31638dba10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f31638ad3d0>,\n",
       " <keras.layers.core.Activation at 0x7f31638ad6d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31637c9190>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3163745250>,\n",
       " <keras.layers.merge.Add at 0x7f31637621d0>,\n",
       " <keras.layers.core.Activation at 0x7f31636e5cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3163695e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f316364ab90>,\n",
       " <keras.layers.core.Activation at 0x7f316364a7d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f31635fb250>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f316355dc10>,\n",
       " <keras.layers.core.Activation at 0x7f31634fccd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3163494d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f316338ca10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3163475650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f316334b050>,\n",
       " <keras.layers.merge.Add at 0x7f316328f4d0>,\n",
       " <keras.layers.core.Activation at 0x7f31632a3ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f316323a090>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f316320cf10>,\n",
       " <keras.layers.core.Activation at 0x7f316320cc10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3163140390>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3163128f10>,\n",
       " <keras.layers.core.Activation at 0x7f3163128950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3163057710>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3163036950>,\n",
       " <keras.layers.merge.Add at 0x7f3163036ed0>,\n",
       " <keras.layers.core.Activation at 0x7f3162f54c50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3162f0b190>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3162ed09d0>,\n",
       " <keras.layers.core.Activation at 0x7f3162ed01d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3162e07b10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3162de95d0>,\n",
       " <keras.layers.core.Activation at 0x7f3162de9650>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3162d00750>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3162cebf50>,\n",
       " <keras.layers.merge.Add at 0x7f3162ceb990>,\n",
       " <keras.layers.core.Activation at 0x7f3162c1b110>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7f3162f544d0>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7f31c59754d0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 14:44:55.201313: W tensorflow/core/framework/allocator.cc:107] Allocation of 321126400 exceeds 10% of system memory.\n",
      "2024-12-09 14:45:00.781926: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n",
      "2024-12-09 14:45:06.858466: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n",
      "2024-12-09 14:45:14.838851: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n",
      "2024-12-09 14:45:21.998745: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 55/101 [===============>..............] - ETA: 59:33 - loss: 0.1106 - acc: 0.9633  "
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "prev_pub_hash": "cf2970a1d2c549fe86023eaa076d0ce4936c4275baf2cccfdad8fe6ce3a8a6c2"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
